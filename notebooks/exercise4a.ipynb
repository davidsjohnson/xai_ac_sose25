{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidsjohnson/xai_ac_sose25/blob/main/notebooks/exercise4a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_jlgYf-fjuD"
      },
      "source": [
        "# XAI for Affective Computing (SoSe2025)\n",
        "# Exercise 4a: Concept-Based Explanations of Facial Expression Recognition\n",
        "\n",
        "In this notebook you will attempt to generate concept-based explanations for a facial expression recognition (FER) CNN trained on raw image data, using a subset of the [AffectNet dataset](http://mohammadmahoor.com/affectnet/).\n",
        "\n",
        "We will use a concept-based approach to generating explanations in this notebook. To do this we will use Concept Relevance Propagation (CRP), which we learned about in the paper [\"From attribution maps to human-understandable explanations through Concept Relevance Propagation\".](https://www.nature.com/articles/s42256-023-00711-8)\n",
        "\n",
        "The documentation of the librqary is still limited but the [CRP GitHub Repo](https://github.com/rachtibat/zennit-crp) has enough to get use started.  So make sure to review the README.  \n",
        "\n",
        "To use this notebook, please make sure to go step by step through each of the cells review the code and comments along the way.\n",
        "\n",
        "***NOTE**: This notebook runtime could be improved by using a GPU if available.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfJfxYWNfjuF"
      },
      "source": [
        "## Notebook Setup\n",
        "\n",
        "Make sure to set to Colab flag below before running the code based on the environment you are using.\n",
        "\n",
        "If you are running the notebook locally make sure to update the python packages by running `pip install -r requirements.txt` at the command line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colab = False # set to True if running in Google Colab or False if running locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo0rt7D7eCE6",
        "outputId": "abb6d6bb-395a-4106-ca50-00425cbe46bd"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "  !git clone https://github.com/davidsjohnson/xai_ac_sose25.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YYhviWyeCE8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if colab:\n",
        "  sys.path.append(os.path.realpath('xai_ac_sose25'))\n",
        "else:\n",
        "  sys.path.append(os.path.realpath('../'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrIlE77OeCE8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "import utils\n",
        "import img_utils\n",
        "import models\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN-pi0V_fjuH",
        "outputId": "faef548d-3797-4e16-fb79-fbe7d6dc75af"
      },
      "outputs": [],
      "source": [
        "base_dir = Path('../data/') if not colab else Path('xai_ac_sose25/data/')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download the AffectNet dataset extracted features and a sample set of images for visualization\n",
        "affnet_dir = utils.download_file('https://uni-bielefeld.sciebo.de/s/EmfF9r93LG4jcT9/download',\n",
        "                          file_name='affectnet_data.zip',\n",
        "                          cache_dir=base_dir,\n",
        "                          extract=True,\n",
        "                          force_download=False,     # set to False if you have already downloaded the dataset\n",
        "                          archive_folder='affectnet_data')\n",
        "affnet_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nqMJsz7eCFC"
      },
      "source": [
        "## XAI for FER with  Convoluational Neural Nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKUaUmkbfjuL"
      },
      "source": [
        "### Setup the Pytorch Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thzbWv5PeCFD"
      },
      "outputs": [],
      "source": [
        "#class labels\n",
        "class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']\n",
        "\n",
        "# Setup XAI Data from AffectNet Deep Learning Model\n",
        "TRAIN_MEAN = [0.485, 0.456, 0.406]\n",
        "TRAIN_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# trainsform to preprocess the images\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=TRAIN_MEAN, std=TRAIN_STD),\n",
        "    transforms.Resize((224, 224))\n",
        "])\n",
        "\n",
        "# set up and load the dataset\n",
        "data_dir = base_dir / 'affectnet_data/affectnet/val_class'\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=test_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=80, shuffle=False)\n",
        "\n",
        "# load the images for visualization\n",
        "images = [Image.open(f[0]).convert('RGB').resize((224,224)) for f in dataset.imgs] # load images as PIL objects and resize them\n",
        "images = [np.array(img) / 255.0 for img in images] # convert to numpy arrays and rescale for display\n",
        "\n",
        "# get the true labels and class names\n",
        "y_true = np.array([f[1] for f in dataset.imgs])\n",
        "y_labels = [class_names[f[1]] for f in dataset.imgs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfZriG2RfjuL"
      },
      "source": [
        "### Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7ClFd4LeCFD",
        "outputId": "4fd2c6c5-8c40-45f4-8e64-3290818b9364"
      },
      "outputs": [],
      "source": [
        "# download checkpoint\n",
        "ckpt_link = 'https://uni-bielefeld.sciebo.de/s/0tAa2wPhGxSDjbM/download'\n",
        "ckpt_path = utils.download_file(ckpt_link,\n",
        "                                'affectnet.pth',\n",
        "                                cache_dir= base_dir / 'affectnet/model',\n",
        "                                extract=False,\n",
        "                                force_download=False\n",
        "                                )\n",
        "ckpt_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFYi82jieCFD",
        "outputId": "dae213cc-8737-428c-b39b-8156b93e23f5"
      },
      "outputs": [],
      "source": [
        "model = models.ResNet18(n_classes=len(class_names), pretrained=True)\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JGq3R13fjuM"
      },
      "source": [
        "### Evaluation of Model\n",
        "\n",
        "This model performs much better than the AU dataset, with around $60\\%$ accuracy.  Stil not great but this is pretty close the state-of-the-art for the AffectNet dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDO-WZYOeCFE",
        "outputId": "763fc6db-ff51-487a-855e-c6613931f96b"
      },
      "outputs": [],
      "source": [
        "inverse_weights = torch.from_numpy(1.0/np.array([74874, 134415, 25459, 14090, 6378, 3803, 24882, 3750])).type(torch.float32).to(device)\n",
        "loss = torch.nn.CrossEntropyLoss(weight=inverse_weights)\n",
        "_, _, y_preds, probs = evaluate.evaluate_model(model, dataloader, loss, device=device)\n",
        "\n",
        "y_preds = np.array(y_preds)\n",
        "# validate predictions and true values\n",
        "(y_preds == y_true).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iqD1WTsfjuM"
      },
      "source": [
        "## Generate Explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHsrPdR1zzkj",
        "outputId": "748fd5a4-3a99-4765-94bd-5bf3b8f06185"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "  !pip install -q zennit-crp[fast_img]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxbPvqo-gR_M"
      },
      "outputs": [],
      "source": [
        "from crp.attribution import CondAttribution\n",
        "from crp.concepts import ChannelConcept\n",
        "from crp.helper import get_layer_names\n",
        "\n",
        "from zennit.composites import EpsilonPlusFlat\n",
        "from zennit.canonizers import SequentialMergeBatchNorm\n",
        "from zennit.torchvision import ResNetCanonizer\n",
        "\n",
        "from crp.visualization import FeatureVisualization\n",
        "from crp.image import plot_grid, imgify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqo0zHgfwcoY"
      },
      "source": [
        "### Task 1 - Generate CRP Attribution Maps\n",
        "\n",
        "[CRP GitHub Repo](https://github.com/rachtibat/zennit-crp)\n",
        "\n",
        "Review the [Attributions Tutorial](https://github.com/rachtibat/zennit-crp/blob/master/tutorials/attributions.ipynb) for info on CRP and help with these tasks\n",
        "\n",
        "**Task 1.1:** Generate a basic feature attribution map for one example from the test data.  The feature attribution should be conditioned on just the predicted class.  This will provide us with a standard saliency map and is equivilant to LRP.\n",
        "\n",
        "\n",
        "**Task1.2:** Generate attribution maps for three randomly selected \"concepts\" from the last layer of the network.  You can use the `get_layer_names` function to find the name of the last layer of our model.  Each \"concept\" defined as individual feature map from that layer. In our model the last layer has 512 feature maps, so just choose 3 random feature maps to use in your conditions for generating attribution values.  Then visualize the attibution maps.\n",
        "\n",
        "\n",
        "You can use the \"Broadcast\" functionality described in the tutorial to do this.\n",
        "\n",
        "Now the generated attribution maps represent the pixels of the image most important to that specific feature map.  (But note, that we do not yet now how important these feature maps are since we just randomly selected them)\n",
        "\n",
        "**Task 1.3:** Identify the top 5 concepts (i.e. feature maps) from the last layer of the network for your selected image's predicted class.  Then plot their corresponding feature maps.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN7iDMrcfjuM"
      },
      "source": [
        "**Preview the Dataset with Predictions**\n",
        "\n",
        "The code below will display images from the XAI dataset.\n",
        "- Try changing value of `start` to get a new set of images (there are 10 images for each class; for example, the class happy will be at indexes 10-19)\n",
        "- Search through the images to find some that might be interesting to Explain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l3m1zHSfjuM"
      },
      "outputs": [],
      "source": [
        "start=40\n",
        "img_utils.display_nine_images(images, y_true, y_preds, start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxQMSkJ5fjuM"
      },
      "outputs": [],
      "source": [
        "####### Select Your image #######\n",
        "##################################\n",
        "\n",
        "idx = \n",
        "cls = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxKhAstBhznd"
      },
      "outputs": [],
      "source": [
        "# get sample and visualize it\n",
        "sample = images[idx]\n",
        "sample = test_transform(sample)\n",
        "sample = sample.unsqueeze(0)\n",
        "sample = sample.to(torch.float).to(device)\n",
        "imgify(sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKOaZ7yAtGpo"
      },
      "outputs": [],
      "source": [
        "###### Enter your Code Below ######\n",
        "##################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSnrDDUcuSh4"
      },
      "source": [
        "### Task 2 - Relevance Maximization\n",
        "\n",
        "Relvance Maximation aims to identify the top images that maximize the relevence score for a given concept. The idea is to find a subsample of the dataset that helps to visually understand what is the human-understandable concept the model learned for that model concept.  \n",
        "\n",
        "The [Feature Visualization Tutorial Notebook](https://github.com/rachtibat/zennit-crp/blob/master/tutorials/feature_visualization.ipynb) will help you with this task.  \n",
        "\n",
        "**Tasks 2.1:** Using the previously identified top 10 concept ids, use the `FeatureVisualization` class to find the images that maximize the relevance values of each concept. Then plot the images using the `plot_grid` function.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAhmEUbhQGXR"
      },
      "outputs": [],
      "source": [
        "###### Enter your Code Below ######\n",
        "##################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxaVH_JQK2u"
      },
      "source": [
        "### Task 3 - CRP Questions\n",
        "\n",
        "Answer the questions below\n",
        "\n",
        "\n",
        "**Q 1.1**  \n",
        "Explain the purpose of the Composites and Canonizers from the Zennit package. Which composite did you select and why? Do the results look similar if you select another composite?\n",
        "\n",
        "**Q 1.2**  \n",
        "Try to semantically describe the identified concepts based on the images selected via RelMax.  Do you find clear \"concepts\" in the indentified images for of the top feature maps?\n",
        "\n",
        "**Q 1.3**   \n",
        "Is it as easy and straightforword to describe the concepts as they suggest in the original paper?\n",
        "\n",
        "**Q 1.4:**  \n",
        "How do these results compare with teh saliency maps from SHAP and Integrated Gradients?\n",
        "\n",
        "**Q 1.5:**  \n",
        "How might use CRP and RelMax to get a more detailed understanding of the different layers in the network?\n",
        "\n",
        "**Q 1.6:**  \n",
        "Can you think of an approach that would integrate facial action units into the idenitification of semantic concepts from RelMax?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaVRa1K5RVs1"
      },
      "source": [
        "Write your answer here..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
