{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidsjohnson/xai_ac_sose25/blob/main/notebooks/exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_QGK4mPhdpB"
      },
      "source": [
        "# XAI for Affective Computing (SoSe2025)\n",
        "# Exercise 2: Local Explanations of Facial Expression Recognition with Action Units\n",
        "\n",
        "In this notebook you will attempt to generate local explanations for predictions of tree-based models trained on Facial Action Units extracted from the AffectNet dataset, see Exercise 1 notebook for more details.  \n",
        "\n",
        "We will use a variety of different methods to generate local explanations, including (but not limited to) [LIME](https://github.com/marcotcr/lime) and [SHAP](https://shap.readthedocs.io/en/latest/), two of the most common approaches used for local explanations on tabular data.  \n",
        "\n",
        "To use this notebook, please make sure to go step by step through each of the cells review the code and comments along the way.\n",
        "\n",
        "\n",
        "**NOTE**: If using Google Colab, you can use a CPU only runtime as the model is not optimized for GPU acceleration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuY-9QUchdpD"
      },
      "source": [
        "## Notebook Setup\n",
        "\n",
        "Make sure to set the `colab` flag when using Google Colab so all necessary packages and paths are setup properly.\n",
        "\n",
        "If not using Colab, make sure to update the required packages by running `pip install -r requirements` in local virtual environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colab = False # set to False if running locally or True if using Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo0rt7D7eCE6",
        "outputId": "b0490def-4f82-4499-c5a8-52b45451e533"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "  print(\"Running on Google Colab and installing dependencies\")\n",
        "  !git clone https://github.com/davidsjohnson/xai_ac_sose25.git\n",
        "  # fix xgboost incompatiblity issue\n",
        "  %pip uninstall -y -q scikit-learn\n",
        "  %pip install -q scikit-learn==1.5.2\n",
        "  %pip install -q interpret\n",
        "  %pip install -q lime\n",
        "  %pip install -q shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YYhviWyeCE8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "if colab:\n",
        "  sys.path.append(os.path.realpath('xai_ac_sose25'))\n",
        "else:  \n",
        "  sys.path.append(os.path.realpath('../'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrIlE77OeCE8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "import utils\n",
        "import img_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McVonSDNhdpF"
      },
      "outputs": [],
      "source": [
        "base_dir = Path('../data/') if not colab else Path('wise24_xai_ac/data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download the AffectNet dataset extracted features and a sample set of images for visualization\n",
        "affnet_dir = utils.download_file('https://uni-bielefeld.sciebo.de/s/EmfF9r93LG4jcT9/download',\n",
        "                          file_name='affectnet_data.zip',\n",
        "                          cache_dir=base_dir,\n",
        "                          extract=True,\n",
        "                          force_download=False,     # set to False if you have already downloaded the dataset\n",
        "                          archive_folder='affectnet_data')\n",
        "affnet_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnMF5LMJeCE9"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlsNWhJbhdpG"
      },
      "source": [
        "### Loading Images and Action Units\n",
        "\n",
        "In the next cell, you will load csv files that contain extracted actions units and paths to the corresponding images.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H2p0wSfeCE_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Full data from training and evaluation\n",
        "train_csv = affnet_dir / 'affectnet_aus/train_aus.csv'\n",
        "val_csv = affnet_dir / 'affectnet_aus/val_aus.csv'\n",
        "\n",
        "# load training and validation data as pandas dataframes\n",
        "# action units extracted from the AffectNet dataset via OpenFace\n",
        "df_train = pd.read_csv(train_csv)\n",
        "df_val = pd.read_csv(val_csv)\n",
        "\n",
        "# smaller dataset for explanations\n",
        "xai_csv = affnet_dir / 'affectnet_aus/eval_aus.csv'\n",
        "df_xai = pd.read_csv(xai_csv)\n",
        "df_xai['image'] = df_xai['image'].str.replace('../data', str(affnet_dir))\n",
        "\n",
        "# get the class labels\n",
        "class_names = ['Neutral', 'Happy', 'Sad', 'Surprise', 'Fear', 'Disgust', 'Anger', 'Contempt']  # same class labels as before\n",
        "\n",
        "# Gets all images from folder used for XAI tasks\n",
        "images = [io.imread(f) for f in df_xai.image]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV3o8Op0rMJ6"
      },
      "source": [
        "Here is a feature name map to make AUs values easier to Read in Shap Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIfetw06rK9m",
        "outputId": "d760e46f-7044-475b-a291-8584e3ddc7e9"
      },
      "outputs": [],
      "source": [
        "# Mapping of Action Units (AUs) to their names\n",
        "AU_MAP = {\n",
        "    \"AU01\": \"Inner Brow Raiser\",\n",
        "    \"AU02\": \"Outer Brow Raiser\",\n",
        "    \"AU04\": \"Brow Lowerer\",\n",
        "    \"AU05\": \"Upper Lid Raiser\",\n",
        "    \"AU06\": \"Cheek Raiser\",\n",
        "    \"AU07\": \"Lid Tightener\",\n",
        "    \"AU09\": \"Nose Wrinkler\",\n",
        "    \"AU10\": \"Upper Lip Raiser\",\n",
        "    \"AU12\": \"Lip Corner Puller\",\n",
        "    \"AU14\": \"Dimpler\",\n",
        "    \"AU15\": \"Lip Corner Depressor\",\n",
        "    \"AU17\": \"Chin Raiser\",\n",
        "    \"AU20\": \"Lip Stretcher\",\n",
        "    \"AU23\": \"Lip Tightener\",\n",
        "    \"AU25\": \"Lips Part\",\n",
        "    \"AU26\": \"Jaw Drop\",\n",
        "    \"AU28\": \"Lip Suck\",\n",
        "    \"AU45\": \"Blink\",\n",
        "}\n",
        "\n",
        "# Generate mappings for both 'r' (regression) and 'c' (classification) versions\n",
        "AU_FEATURE_MAP = {f\"{au}_r\": f\"{au} - {name} - R\" for au, name in AU_MAP.items()}\n",
        "AU_FEATURE_MAP.update({f\"{au}_c\": f\"{au} - {name} - C\" for au, name in AU_MAP.items()})\n",
        "\n",
        "# Print the mapping\n",
        "print(AU_FEATURE_MAP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# update the feature names in the dataframe to be more descriptive\n",
        "df_train.rename(columns=AU_FEATURE_MAP, inplace=True)\n",
        "df_val.rename(columns=AU_FEATURE_MAP, inplace=True)\n",
        "df_xai.rename(columns=AU_FEATURE_MAP, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get only the columns storing action units from the dataframe\n",
        "# there are also facial landmarks and other features in the dataset could be useful\n",
        "# but we igore them for now and focus on action units\n",
        "feature_names = [col for col in df_val.columns if col.startswith('AU')]\n",
        "numerical_features = [feat for feat in feature_names if '- R' in feat]\n",
        "numerical_idxs = [i for i, feat in enumerate(feature_names) if '- R' in feat]\n",
        "categorical_features = [feat for feat in feature_names if '- C' in feat]\n",
        "categorical_idxs = [i for i, feat in enumerate(feature_names) if '- C' in feat]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICf37_6teCE_"
      },
      "source": [
        "## Model Training and Evaluation\n",
        "\n",
        "First, let's load the data and the trained models. Then we will evaluate the model peformance, before we start with the explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX-WtgK9eCFA"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wk79Re8hdpI"
      },
      "source": [
        "### Setup the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_biGmc9_eCFA",
        "outputId": "be75af35-9023-48f4-e183-65a36692721e"
      },
      "outputs": [],
      "source": [
        "# we will only use numerical features to remove collinearity with categorical features\n",
        "X_train = df_train[numerical_features]\n",
        "y_train = df_train['class']\n",
        "X_test = df_val[numerical_features]\n",
        "y_test = df_val['class']\n",
        "\n",
        "X_xai = df_xai[numerical_features]\n",
        "y_xai = df_xai['class']\n",
        "\n",
        "print('Training data shape:', X_train.shape, y_train.shape)\n",
        "print('Test data shape:', X_test.shape, y_test.shape)\n",
        "print('XAI data shape:', X_xai.shape, y_xai.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBuske61hdpI"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "Now let's train the XGBoost Model on the split dataset. The accuracy of the model in the training data should be around $92\\%$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_ngk-0DeCFC",
        "outputId": "adf246f0-d548-4bc5-ec73-23b2de6789ba"
      },
      "outputs": [],
      "source": [
        "# # Train model from scratch\n",
        "random_state = 10\n",
        "clf = XGBClassifier(max_depth=20, eta=0.1, reg_lambda=3, random_state=random_state)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhSP25hchdpJ"
      },
      "source": [
        "### Evaluate the Model\n",
        "\n",
        "Now we will evaluate it on the test dataset. Unfortunately, the accuracy is only $40\\%$ but this is still well above chance guessing which would be $1 / 8 * 100 = 12.5\\%$ accuracy (since there are 8 total classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKMpGJm-hELJ",
        "outputId": "b76d8307-8673-40a0-9830-7d8ff52e98a6"
      },
      "outputs": [],
      "source": [
        "# get model predictions\n",
        "y_test_preds = clf.predict(X_test)\n",
        "y_test_true = y_test\n",
        "\n",
        "# eval results\n",
        "print(classification_report(y_test_true, y_test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uBrGhMhhdpL"
      },
      "source": [
        "### Setup XAI dataset and Predictions\n",
        "\n",
        "Evaluate the model and get predictions for the XAI subset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyKf6mMyhdpL",
        "outputId": "9bc46a48-be33-4818-c21a-846f1e14b7af"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on XAI data\n",
        "y_xai_preds = clf.predict(X_xai)\n",
        "y_xai_true = y_xai\n",
        "\n",
        "df_xai['xgb_pred'] = y_xai_preds\n",
        "\n",
        "print(classification_report(y_xai_true, y_xai_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MPI378nhdpL"
      },
      "source": [
        "**Preview the Dataset with Predictions**\n",
        "\n",
        "The code below will display images from the XAI dataset.\n",
        "- Try changing value of `start` to get a new set of images (there are 10 images for each class; for example, the class happy will be at indexes 10-19)\n",
        "- Search through the images to find some that might be interesting to Explain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i_MBrSrLhdpL",
        "outputId": "e599a193-a781-4a00-9209-44d36ce1f160"
      },
      "outputs": [],
      "source": [
        "start=70\n",
        "img_utils.display_nine_images(images, df_xai['class'], df_xai['xgb_pred'], start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Tasks A: Generate Local Explanations\n",
        "\n",
        "Following the methods we used in the last exercise, we will now generate local explanation for individual predications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Generate Explanations from an Interpretable Model\n",
        "\n",
        "In this task, you will generate and review local explanations using the interpretable model from Exercise 1, [an Explainable Boosting Machine (EBM)](https://interpret.ml/docs/ebm.html). The code for training the model is already provided so you will only need to generate the local explanations. \n",
        "\n",
        "1. First, select 6 images from the XAI dataset above, 3 correctly predicted images and 3 incorrect.\n",
        "   - (store the image indexes, as you will use these in the upcoming tasks as well)\n",
        "2. Use the built-in functionality from the EBM to show the local explanations for the 6 explanations.\n",
        "3. Answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "from interpret import show "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ebm = ExplainableBoostingClassifier(reg_lambda=3)\n",
        "ebm.fit(X_train, y_train)\n",
        "ebm.score(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get model predictions\n",
        "y_test_preds = ebm.predict(X_test)\n",
        "y_test_true = y_test\n",
        "\n",
        "# eval results\n",
        "print(classification_report(y_test_true, y_test_preds, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Your Code Here #####\n",
        "##########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 1 Questions\n",
        "\n",
        "1. How are the local explanations generated from the EBM? What makes this method inherently interpretable?\n",
        "2. How are the local explanation plots interpreted?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Generate individual conditional expectation (ICE) plots\n",
        "\n",
        "Individual conditional expectation (ICE) plots are similar to PDP, but show how an individual instance's prediction changes when a feature changes. See https://christophm.github.io/interpretable-ml-book/ice.html for a more indepth description.\n",
        "\n",
        "In this task, you will create ICEs for the XGB model using the Scikit-Learn via the PDP implementation: https://scikit-learn.org/stable/modules/partial_dependence.html.\n",
        "\n",
        "\n",
        "1. Using the same emotion classes and features combinations from Exercise 1, generate ICEs using the XAI data subset. You can also include the PDP in the visualization. \n",
        "2. Then answer the questions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.inspection import PartialDependenceDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Your Code Here #####\n",
        "##########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task 2 Questions\n",
        "\n",
        "1. What do the individual ICE lines represent? And how do they relate the the PDP?\n",
        "2. What do the explanations tell you about each feature?  \n",
        "3. Do the explanations alighn with the expected AUs explained on the iMotions website regarding \"Emotions and Action Units\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Tasks B: Generate Explanations with LIME\n",
        "\n",
        "Local Interpretable Model-agnostic Explanations (LIME) is a widely used method for generating local explanations by learning an interpretable model in the vicinity of a specific instance. It does so by perturbing the instance to create a neighborhood of similar samples, which are then used to train the local surrogate model. You can read more about it in the original paper: [“Why Should I Trust You?” Explaining the Predictions of Any Classifier](https://dl.acm.org/doi/10.1145/2939672.2939778) by Ribeiro et al. (2016)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lime\n",
        "from lime.lime_tabular import LimeTabularExplainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3: Generate Local LIME Explanations\n",
        "\n",
        "In this task you will use the [LIME Python package](https://github.com/marcotcr/lime) to generate explanations on the AffectNet dataset.  \n",
        "\n",
        "1. First implement a [LimeTabularExplainer](https://lime-ml.readthedocs.io/en/latest/lime.html#module-lime.lime_tabular), you can review the [LIME tutorial](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html) for help.\n",
        "2. Generate LIME explanations for your 6 selected data instances, using the `LimeTabularExplainer` and plot the explanations for each data instance (see tutorial mentioned above).  \n",
        "    - HINT: Before showing an explanation, plot the image using `utils.display_one_image()` utility function.  Use `plt.show()` immediately after calling `utils.display_one_image()` to display the image before the explanation charts.\n",
        "    - Make sure to print out the **True** and **Predicted** labels for each instance.\n",
        "    - Try experimenting with different parameters for the explainer and explanation.\n",
        "3. Answer the questions below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Your Code Here #####\n",
        "##########################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task Questions\n",
        "\n",
        "1. Explain in more detail how LIME generates the local neighborhood and how this is used to create a local model used for explanations.  \n",
        "2. What do the generated explanations reveal about the models predictions? Do they align with the expected action units for the given emotion (see \"Emotions and Action Units\": https://imotions.com/blog/learning/research-fundamentals/facial-action-coding-system/)?\n",
        "3. In your opinion, how understandable are the explanations? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your answers here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Tasks C: Generate Explanations with SHAP\n",
        "\n",
        "SHAP (SHapley Additive exPlanations) is a popular method for generating local explanations by assigning each feature a contribution value based on Shapley values from game theory. It explains a model’s prediction by considering all possible combinations of feature contributions. We've already read about the SHAP Tree Explainer; you can also read more about SHAP in the original paper that provides a more generalized description of the method: [“A Unified Approach to Interpreting Model Predictions”](https://arxiv.org/abs/1705.07874) by Lundberg and Lee (2017).\n",
        "\n",
        "In this task you will generate both global and local explanations for the XGBoost model using SHAP. Review the [SHAP Documentation for Details](https://shap.readthedocs.io/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EwkKUfrhdpM"
      },
      "outputs": [],
      "source": [
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h1zVR4q5XhN"
      },
      "source": [
        "### Task 4 - Generate Local SHAP Explanations\n",
        "\n",
        "First, generate local explanations, as they from the basis of the global explanations.\n",
        "\n",
        "1. Generatel local explanations for each of your six images using a Waterfall plot. \n",
        "    - For each incorrect image, make sure to plot both the explanation for the prediction and the true emotion. \n",
        "    - When displaying SHAP plot, also show the original image along side it\n",
        "2.  Generate local explanations using another type of plot from the SHAP library.\n",
        "3. Answer the questions below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##### Your Code Here #####\n",
        "##########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Task Questions\n",
        "\n",
        "1. Which plot type (the Waterfall or your selected one) is more intuitive to you and why?\n",
        "2. How should you interpret the SHAP values presented in the explanations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your answers here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH05WBJ_s9DU"
      },
      "source": [
        "### Task 5 - Generate Global SHAP Explanations\n",
        "\n",
        "Now, in this task you will generate Global SHAP Explanations using the `X_xai` data subset.\n",
        "\n",
        "1. Generate explanations using the `X_xai` dataset and then plot the global explanations for each class of the AffectNet dataset using a `summary_plot` (which should be the same as the `beeswarm_plot`). \n",
        "    - The XGBoost tutorial in the SHAP documentation is a good place get started.  Keep in mind that Facial Expression Recognition is a multi class (8 emotion) problem rather than a binary problem as in the tutorial.\n",
        "2. Then review the [documentation for the different plot types available](https://shap.readthedocs.io/en/latest/api.html#plots), and generate class level explanations using at least one other global explanation type.\n",
        "3. Answer the questions below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-ai9dV8yp-N"
      },
      "outputs": [],
      "source": [
        "###### Enter your Code Here ######\n",
        "##################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OaPjCtGzLnv"
      },
      "source": [
        "#### Task Questions\n",
        "\n",
        "1. How do you interpret the summary plot (ie beeswarm plot) for local explanations?  How do you interpret the plot of your choosing?\n",
        "1. How do you interpret the beeswarm plot (ie beeswarm plot)?  How do you interpret the global explanation plot of your choosing?\n",
        "2. How do SHAP global explanations compare to the methods from Exercise 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your answers here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Questions\n",
        "\n",
        "1. Of the local explanation types, which method did you find the most interpretable?  Why?\n",
        "2. Do the explanations for each of the local approaches align for the same prediction? What does this tell us about the methods? How do you interpret these differences?\n",
        "3. In general, do you find that the model predictions align with the expected AUs for the ground truth and predicted emotions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your answers here..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
